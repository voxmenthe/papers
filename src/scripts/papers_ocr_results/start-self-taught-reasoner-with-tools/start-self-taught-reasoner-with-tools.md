# START: Self-taught Reasoner with Tools

*Source: [arXiv:2503.04625](https://arxiv.org/abs/2503.04625)*

*[Submitted on Thu, 6 Mar 2025 17:11:51 UTC]*

# START: Self-taught Reasoner with Tools 

Chengpeng Li ${ }^{1,2}$, Mingfeng Xue ${ }^{2 *}$, Zhenru Zhang ${ }^{2}$, Jiaxi Yang ${ }^{2+}$, Beichen Zhang ${ }^{2+}$, Xiang Wang ${ }^{1}$, Bowen Yu ${ }^{2}$, Binyuan Hui ${ }^{2}$, Junyang Lin ${ }^{2}$, Dayiheng Liu ${ }^{2 \dagger}$<br>${ }^{1}$ University of Science and Technology of China<br>${ }^{2}$ Alibaba Group<br>\{lichengpeng.lcp,liudayiheng.ldyh\}@alibaba-inc.com


#### Abstract

Large reasoning models (LRMs) like OpenAIo1 and DeepSeek-R1 have demonstrated remarkable capabilities in complex reasoning tasks through the utilization of long Chain-ofthought (CoT). However, these models often suffer from hallucinations and inefficiencies due to their reliance solely on internal reasoning processes. In this paper, we introduce START (Self-Taught Reasoner with Tools), a novel tool-integrated long CoT reasoning LLM that significantly enhances reasoning capabilities by leveraging external tools. Through code execution, START is capable of performing complex computations, self-checking, exploring diverse methods, and self-debugging, thereby addressing limitations of LRMs. The core innovation of START lies in its selflearning framework, which comprises two key techniques: 1) Hint-infer: We demonstrate that inserting artificially designed hints (e.g., "Wait, maybe using Python here is a good idea.") during the inference process of a LRM effectively stimulates its ability to utilize external tools without the need for any demonstration data. Hint-infer can also serve as a simple and effective sequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning (HintRFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and modifying the reasoning trajectories with tool invocation generated by a LRM via Hint-infer, followed by finetuning the LRM. Through this framework, we have fine-tuned the QwQ-32B-Preview model to achieve the START. On PhD-level science QA (GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the competition-level code benchmark (LiveCodeBench), START achieves accuracy rates of $63.6 \%, 95.0 \%, 66.7 \%, 47.1 \%$, and $47.3 \%$, respectively. It significantly outperforms the base QwQ-32B-Preview and achieves performance comparable to the state-of-the-art open-


[^0]weight model R1-Distill-Qwen-32B and the proprietary model o1-Preview.

## 1 Introduction

The evolution of reasoning capabilities in large language models (LLMs) has followed a paradigm shift marked by increasingly sophisticated thinking patterns. The chain-of-thought (CoT) approach (Wei et al., 2022) pioneers this progression by enabling models to decompose problems through explicit intermediate reasoning steps. Then, the breakthrough comes with reinforcement learning exemplified by OpenAI-o1 (OpenAI, 2024b) and DeepSeek-R1 (DeepSeek-AI, 2025), establishing a new paradigm termed long CoT, which emulates human-like cognitive strategies including self-refine, self-reflection, multi-strategy exploration and so on. Despite these advancements, fundamental limitations persist in long Chain-ofthought (CoT) approaches, such as hallucinations when facing complex computations or simulations, primarily due to their exclusive dependence on internal reasoning mechanisms (Li et al., 2025). Tool-integrated reasoning (TIR) (Gou et al., 2024), another approach that improves traditional CoT through tool invocation(typically code interpreter), can effectively mitigate the issues that arise in Long CoT. OpenAI o1 (OpenAI, 2024b) reported that they trained o1 to use external tools, especially for writing and executing code in a secure environment, but they did not provide any technical details on how this is achieved. Therefore, a straightforward yet important question arises: How can we synergistically combine long CoT with TIR?

In this paper, we focus exclusively on Python interpreter invocation, as it is both important and representative of many reasoning tasks (Shao et al., 2024; Yang et al., 2024). The fundamental challenge lies in synthesizing data that includes calls to a Python interpreter within Long Chain-of-thought (CoT). We have tried using direct prompt, well-


[^0]:    *Work done during internships at Alibaba Group.
    ${ }^{\dagger}$ Corresponding author

![img-0.jpeg](img-0.jpeg)

Figure 1: Training framework for START. Training Framework for START. START's training involves two phases: Hint-RFT followed by RFT. a) Hint-infer: code/math data is processed by QwQ-32B-Preview, with responses truncated at predefined terminators. Context-aware hints from a Hint-Library are injected at truncation points (including endpoints), and QwQ-32B-Preview resumes inference using a code interpreter for Python execution feedback. b) Hint-RFT: Hint-infer outputs undergo rule-based scoring, filtering, and content modification to create $D_{\text {seed }}$. QwQ-32B-Preview is then fine-tuned on $D_{\text {seed }}$ to produce START-0, enabling self-aware tool usage. c) RFT: START-0 generates self-distilled trajectories to build $D_{\text {START }}$ (enhancing diversity/tool-use patterns), followed by fine-tuning to produce START.
designed prompt (Li et al., 2025), and in-context prompt (Gou et al., 2024; Schick et al., 2023) on AIME24 and LivecodeBench with QwQ-32BPreview (Qwen Team, 2024) and DeepSeek-R1, but none were successful in prompting the model to invoke the Python tool during the long CoT(see more in Appendix A.3). A possible reason is that large reasoning models(LRMs) typically focus solely on problem-solving during training for complex reasoning tasks, resulting in a loss of generalization in instruction following. Considering the nature of next-token prediction in LRMs, we attempt to insert some hints directly during or at the end of the LRMs' reasoning process, aiming to directly prompt the model to write code and invoke code interpreter. We are surprised to discover that LLMs indeed possess the corresponding capabilities. For mathematical tasks, simply inserting basic hints along with Python identifiers enables the LLM to follow the hints and write the appropriate code. In contrast, for coding generation tasks, carefully designed hints and code templates are necessary to activate the model's ability to execute candidate code on test cases on its own during
the long CoT. We refer to the paradigm of LRM inference aided by hints as Hint-infer.

Based on above Hint-infer, we present START: Self-Taught Reasoner with Tools, a LRM that synergizes Long CoT and TIR, which we refer to as Long TIR. The whole traing framework is illustrated in Figure1. First, we design a set of hints with different functionalities based on the cognitive characteristics of LLMs, which we refer to as the Hint-Library. Figure 3 presents some representative hints from the Hint Library. Second, these hints are randomly inserted after certain highfrequency conjunctions, such as "Alternatively" and "Wait" (see more in Figure 4), because these words typically indicate that the model begins to introspect or seek new solutions to the problem ( Li et al., 2025). Additionally, we also add hints before the stop token of long CoT, as this approach provides the LRM with more time to think without disrupting its original reasoning process. We find it intriguing that when hints are added before the stop token of Long Chain-of-thought (CoT), the model exhibits a sequential test time scaling effect; that is, as the thinking time increases, the success

You are given an array of positive integers nams of length $n$. We call a pair of non-negative integer arrays (arr1, arr2) monotonic if:

- arr1 is monotonically non-decreasing, in other words, arr1 $[0] \Leftrightarrow$ arr1 $[1] \Leftrightarrow \ldots \Leftrightarrow$ arr1 $[n-1]$.
- arr2 is monotonically non-increasing, in other words, arr2 $[0] \Leftrightarrow$ arr2 $[1] \Leftrightarrow \ldots \Leftrightarrow$ arr2 $[n-1]$.
- $\operatorname{arr}(|1|+\operatorname{arr} 2|) \Leftrightarrow$ numo $|1|$ for all $0 \Leftrightarrow i \Leftrightarrow n-1$.

Return the count of monotonic pairs. Since the answer may be very large, return it modulo $10^9+7$ Example1 :

- Input: nams $=[2,3,2]$

Output: 4

- Explanation: The good pairs are: $([0,1,1],[2,2,1])([0,1,2],[2,2,0])([0,2,2],[2,1,0])([1,2,2],[1,1,0])$ Example 2: Input: nams $=[5,5,5,5] \quad$ Output: 126

| Alright, I've got this problem to solve. Let's see what it's asking for... |
| :-- |
| $[\ldots]$ Analyze the problem |
| Okay, first things first, I need to understand what constitutes a valid pair (arr1, arr2). |
| $[\ldots]$ Try to solve it |
| This seems a bit tricky. Let me see if I can find a way to transform ... |
| $[\ldots]$ Change the idea |
| But according to the example, there are 4 valid pairs. So where did I go wrong? |
| $[\ldots]$ Self-reflect |
| Solution Code: |
| 0 |
| 1 |
| 2 |
| 3 |
| 4 |
| 5 |
| 6 |
| 7 |
| 8 |
| 9 |
| 10 |
| 11 |
| 12 |
| 13 |
| 14 |
| 15 |
| 16 |
| 17 |
| 18 |
| 19 |

We efficiently count all valid monotonic pairs while handling... Hallucination
![img-1.jpeg](img-1.jpeg)

Figure 2: Comparison between the responses generated by QwQ-32B-Preview and START. This is a question from LiveCodeBench with a difficulty level of "hard". QwQ-32B-Preview employs long-chain CoT with selfreflection and trying different approaches, yet hallucinates during complex test case analysis, leading to flawed solutions. START retains QwQ-32B-Preview's cognitive framework but integrates code execution: (1) Runs code via interpreter, (2) Detects output mismatch, (3) Iteratively analyzes and debugs, and (4) Gives the final solution. See more cases of START in Appendix A. 5
rate of problem-solving also gets higher(see more in 4.5.2). Through a series of data scoring, filtering, modifications, and rejection sampling finetuning (RFT) (Yuan et al., 2023; Dong et al., 2023; Zelikman et al., 2022), we eventually obtain our START from QwQ-32B-Preview. From Figure 2, it can be seen that there is a comparison between the reasoning of QwQ-32B-Preview and START. When encountering a complex case analysis, QwQ-32-Preview generates hallucinations and provides incorrect answers, while START utilizes a code interpreter to self-debug, delivering the correct answer.

Empirical evaluations across a suite of benchmarks encompassing mathematical problemsolving, scientific inquiries, coding challenges, and GPQA tasks demonstrate that START-RL markedly surpasses existing tool-integrated and long CoT models, including QwQ-32B-Preview, o1-mini, and o1-preview in MATH benchmarks. These results underscore the efficacy of integrating external tools into the long CoT framework, highlighting START-RL as the first open-source tool-integrated long CoT reasoning model that sets
a new standard for LLM performance in complex reasoning domains.

In summary, our contributions are threefold:

- We introduce Hint-infer, a simple and effective sequential test-time scaling method for LRMs such as QwQ-32B-Preview and START.
- We introduce Hint-RFT, a self-training framework that enables a large language model (LRM) to teach itself how to utilize code interpreter.
- We present START, the first open-source LRM that utilizes long CoT and code interpreter to address complex reasoning tasks.


## 2 Related Work

Large Language Models have demonstrated remarkable dominance across numerous Natural Language Processing tasks. To enhance the complex reasoning capabilities of LLMs, Wei et al. (2022) introduce Chain-of-Thought (CoT), which incorporates multi-step intermediate reasoning before

![img-2.jpeg](img-2.jpeg)

Figure 3: Hint-Library. Code generation tasks: Debug hint guides test case review and local code validation. The code template is in A.4. Math reasoning: Domain-specific hints (e.g., Complex Calculations, Self-Reflection, Logic Check, Alternative Methods) steer code-aided reasoning behaviors.
![img-3.jpeg](img-3.jpeg)

Figure 4: Word cloud of conjunction frequency statistics from QwQ-32B-Preview infering on $D_{\text {seed }}$.
arriving at final conclusions. CoT exhibits significant advantages across multiple domains, including mathematics, science, and programming. Subsequently, OpenAI (2024b) further explore CoT and propose the Long Chain-of-Thought framework. In Long CoT, LLMs demonstrate advanced cognitive behaviors such as reflection, verification, correction, and multi-path exploration, thereby further enhancing their problem-solving capabilities in complex reasoning tasks. Moreover, Long CoT exhibits excellent test-time scaling properties, where increased computational resources correlate with improved reasoning outcomes. Models like QwQ-32B-Preview (Qwen Team, 2024), DeepSeek-R1 (DeepSeek-AI, 2025), k1.5 (Team et al., 2025),and InternThinker (Cai et al., 2024) have successfully experimented with Long CoT for enhanced reasoning, combining
fine-tuning and Reinforcement Learning to elevate the performance of open-source reasoning models to unprecedented levels. Notably, subsequent models such as Open-R1 (Huggingface, 2025), O1Replication (Qin et al., 2024), S1 (Muennighoff et al., 2025a) and LIMO (Ye et al., 2025) observes significant benefits from Long CoT even in smaller models through simple distillation.

Nevertheless, significant challenges persist, particularly in addressing hallucination phenomena and computational inaccuracies that impede optimal performance. Drawing parallels with human cognition, where external aids such as scratch paper and calculators substantially mitigate computational errors, LLMs can similarly benefit from the integration of auxiliary tools. Research by Shao et al. (2024) demonstrates that codebased pre-training protocols significantly augment LLMs' mathematical reasoning proficiency. Various works successfully implemente Python-based computational tools to enhance model performance (Chen et al., 2023; Gou et al., 2024; Liao et al., 2024; Li et al., 2024). In the domain of mathematical proof verification, the incorporation of Lean yield notable advancements (Xin et al., 2024; Wu et al., 2024).

This study synthesizes the advantages of Python-

based tools and long CoT methodologies, advancing LRMs like QwQ-32B-Preview through the integration of tool utilization capabilities. This integrated approach yields improved performance metrics across mathematical and coding benchmarks.

## 3 Methodology

### 3.1 Training data

Our training data comprises two parts: one consists of math data sourced from previous AIME problems ${ }^{1}$ (before 2024), MATH (Hendrycks et al., 2021), and Numina-MATH (LI et al., 2024), while the other includes code data from Codeforces ${ }^{2}$, code contests ${ }^{3}$ and LiveCodeBench(before July 2024) (Jain et al., 2024). We apply the same decontamination method as described in (Yang et al., 2024) to the training set in order to minimize potential test data leakage risks. There are a total of 40 K math problems and 10 K code problems, and the specific quantity distribution can be referred to in Appendix A.1.

### 3.2 Hint-RFT

Construct Hint we have designed a series of hints(Hint-Library) tailored to the various scenarios that may arise during LLM reasoning. Since mathematical reasoning with tools can be quite complex, we develop different hints focused on reflection, logical verification, exploring new methods, and more. These diverse hints enable the model to adopt different strategies based on the specific situation it encounters. For coding tasks, we concentrate on designing hints that promote the model's self-debugging capabilities. By encouraging the model to check its code against test cases, it can verify the correctness of its solutions and make necessary adjustments as needed. We find that adding code template to the hint can effectively prompt the model to generate desired debugging code. Figure 3 show the hints.

Hint-infer For mathematical reasoning, we strategically insert hints after specific conjunction tokens, such as Alternatively and Wait as these tokens typically indicate that the model may be questioning its own reasoning or considering alternative approaches. It is important to note that

[^0]after the hints are inserted, the model continues its reasoning process. The generated code is then sent to a Python interpreter for execution, and upon obtaining the results, the model proceeds to generate further outputs based on that information. Similarly, we can insert hints before the stop token to encourage the model to engage in deeper reasoning based on its existing reasoning. By inserting hints at these critical junctures at random, we encourage the model to explore a broader reasoning space. For code reasoning, we primarily concentrate on code generation tasks. We insert hints that prompt the model to test its own code right before the model generates the final code solution. This strategic placement encourages the model to engage in self-assessment, thereby enhancing the accuracy and reliability of the generated code. A more intuitive description is in Figure 1.

Data process and model fine-tuning Inspired by (Lightman et al., 2024), we adopt an active learning method, where we perform greedy inference and hint inference using QwQ-32B-Preview on all training data, and we recall data from reasoning tasks where QwQ-32B-Preview would not succeed without tools, but succeeded with hint inference. This is incorporated into our startup data $D_{\text {seed }}$ with 10 K math data and 2 K code data. It is important to note that, in addition to scoring the generated reasoning trajectories based on the rules, we also filter out responses that contain repetitive patterns. Additionally, we modify the Python identifiers in the code data hints to "Debug Code Template" and remove the output placeholders. We fine-tune QwQ-32B-Preview based on $D_{\text {seed }}$ to obtain START-0. The purpose of this fine-tuning step is to enable the model to learn the response paradigm for utilizing tools.

### 3.3 RFT

To further enhance the diversity and quantity of the training data, as illustrated in Figure 1, we utilize the obtained START-0 to perform rejection sampling fine-tuning on all training data. Specifically, we use sampling parameters of temperature 0.6 and top-p 0.95 with START-0 to perform 16 rounds of sampling. We score the sampled long TIR data, filter out responses with repetitive patterns, and manually modify any unreasonable content. We retain a maximum of one response per question, resulting in our dataset $D_{\text {START }}$. Using the 40,000 math data entries and 10,000 code data entries from $D_{\text {START }}$


[^0]:    ${ }^{1}$ https://huggingface.co/datasets/gneubig/ aime-1983-2024
    ${ }^{2}$ https://codeforces.com/problemset
    ${ }^{3}$ https://github.com/google-deepmind/code_ contests

we fine-tune QwQ-32B-Preview once again, resulting in our final LRM named START.

## 4 Experiment

### 4.1 Benchmarks

In this work, we primarily focus on integrating Python tools into long CoT reasoning. Given Python's effectiveness in enhancing computational and programming aspects of reasoning, we select several representative and challenging reasoning benchmarks to validate our methodology.

GPQA: This benchmark comprises 448 graduate-level multiple-choice questions authored by experts in biology, physics, and chemistry (Rein et al., 2023). These questions present significant challenges, as even domain experts achieved less than $75 \%$ accuracy in testing (OpenAI, 2024b).

Math Benchmarks: Mathematical performance of LLMs remains a focal point for researchers. In the mathematical domain, we select MATH500 (Lightman et al., 2024) at the high school level, along with competition-level AMC23 ${ }^{4}$, AIME24 ${ }^{5}$ and AIME25 ${ }^{6}$ as our evaluation datasets. These datasets encompass various mathematical question types, including algebra, calculus, number theory, probability, and geometry, enabling a comprehensive assessment of LLMs' mathematical problem-solving capabilities.

LiveCodeBench: This benchmark evaluates LLMs' programming capabilities, with test cases categorized into easy, medium, and difficult levels (Jain et al., 2024). We choose 112 problems from August 2024 to November 2024 as the code benchmark. These questions are categorized as hard, medium, and easy based on difficulty.

### 4.2 Baselines

We evaluate our approach against the following baseline methods:

General LLMs: These methods are general LLMs without the long CoT reasoning. The open-source models include Qwen2.5-32BInstruct (Yang et al., 2025), Qwen2.5-Coder-32B-Instruct (Hui et al., 2024),DeepSeek-V3671B (DeepSeek-AI et al., 2024), Llama3.3-70B-

[^0]Instruct (Dubey et al., 2024) and GPT-4o (OpenAI, 2024a).

LRMs: These methods are equipped with long CoT reasoning. (1) API only: These models can only be accessed through the API, including o1-series (OpenAI, 2024b) and o3mini (OpenAI, 2025). (2) Open weights: we compare with some open weights LLMs, including DeepSeek-r1 series (DeepSeek-AI, 2025), QwQ-32B-Preview (Qwen Team, 2024), s1 (Muennighoff et al., 2025b) and Search-o1 (Li et al., 2025).

### 4.3 Implementation Details

We fine-tune QwQ-32B-Preview with $D_{\text {START }}$ to get START. We train the base models with key settings including a 7e-6 learning rate, 128 global batch size, a cosine scheduler with a $3 \%$ warmup, a maximum context length of 16,384 tokens and 3 training epochs. Checkpoints are not selected with early stops. The training process employs full-parameter fine-tuning with DeepSpeed ZeRO-3 (Rajbhandari et al., 2020) optimization. Responses are generated using greedy decoding with a maximum sequence length of 32,768 and a limit of 6 maximum tool uses. For all benchmarks, we report the pass@1 performance and the evaluation metric is from (Yang et al., 2024). We use the same training and inference chat template as QwQ-32B-Preview (Qwen Team, 2024). The hardware setup involves 32 NVIDIA A100 GPUs.

### 4.4 Main Results

Table 1 presents the evaluation results of START across various benchmarks, demonstrating its superior reasoning performance in scientific, mathematical, and coding domains compared to other open-source models. Overall, general LLMs, even domain-specific LLMs, are difficult to compete with LRMs in complex tasks.

PHD-level Science QA Performance It can be observed that on the ScienceQA benchmark, START demonstrates an absolute improvement of 5.5\% over QwQ-32B-Preview, achieving the same score as the state-of-the-art model, search-o1-32B. Table 2 presents the scores of QwQ-32B-Preview, Search-o1, and START across three subjects of GPQA: Physics, Chemistry, and Biology. Specifically, START achieves the highest score in Physics, while Search-o1 outperforms QwQ-32B-Preview significantly in Biology. This discrepancy can be


[^0]:    ${ }^{4}$ https://huggingface.co/datasets/AI-MO/
    aimo-validation-amc
    ${ }^{5}$ https://huggingface.co/datasets/AI-MO/
    aimo-validation-aim
    ${ }^{6}$ https://huggingface.co/datasets/TIGER-Lab/
    AIME25

Table 1: Main results on challenging reasoning tasks, including PhD-level science QA, math, and code benchmarks. We report Pass@1 metric for all tasks. For models with 32B parameters, the best results are in bold and the second-best are underlined. Symbol " $\ddagger$ " indicates results from their official releases.

| Method | GPQA | MATH500 | AMC23 | AIME24 | AIME25 | LiveCodeBench |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| General LLMs |  |  |  |  |  |  |
| Qwen2.5-32B | 46.4 | 75.8 | 57.5 | 23.3 | - | 22.3 |
| Qwen2.5-Coder-32B | 33.8 | 71.2 | 67.5 | 20.0 | - | 25.0 |
| Llama3.3-70B | 43.4 | 70.8 | 47.5 | 36.7 | - | 34.8 |
| DeepSeek-V3-671B | 59.1 | 90.2 | - | 39.2 | - | 40.5 |
| GPT-40 ${ }^{\dagger}$ | 50.6 | 60.3 | - | 9.3 | - | 33.4 |
| Reasoning LLMs |  |  |  |  |  |  |
| API Only |  |  |  |  |  |  |
| o1-preview ${ }^{\dagger}$ | 73.3 | 85.5 | 81.8 | 44.6 | 37.5 | 53.6 |
| o1-mini ${ }^{\dagger}$ | - | 90.0 | - | 63.6 | 50.8 | - |
| o1 ${ }^{\dagger}$ | 77.3 | 94.8 | - | 74.4 | - | 63.4 |
| o3-mini(low) ${ }^{\dagger}$ | 70.6 | 95.8 | - | 60.0 | 44.2 | 75.6 |
| Open weights |  |  |  |  |  |  |
| R1-Distill-Qwen-32B ${ }^{\dagger}$ | 62.1 | 94.3 | 93.8 | 72.6 | 46.7 | 57.2 |
| s1-32B ${ }^{\dagger}$ | 59.6 | 93.0 | - | 50.0 | 33.3 | - |
| Search-o1-32B ${ }^{\dagger}$ | 63.6 | 86.4 | 85.0 | 56.7 | - | 33.0 |
| QwQ-32B-Preview | 58.1 | 90.6 | 80.0 | 50.0 | 40.0 | 41.4 |
| START | 63.6(+5.5) | 94.4(+3.8) | 95.0( +15.0 ) | 66.7( +16.7 ) | 47.1(+7.1) | 47.3(+5.9) |

Table 2: Scores on GPQA in various subjects.

| Model | Physics | Chemistry | Biology |
| :-- | :--: | :--: | :--: |
| QwQ-32B-Preview | 73.8 | 41.9 | 68.4 |
| Search-o1 | 77.9 | $\mathbf{4 7 . 3}$ | $\mathbf{7 8 . 9}$ |
| START | $\mathbf{8 0 . 0}$ | $\mathbf{4 7 . 3}$ | 68.4 |

Table 3: Scores on questions of different difficulty levels on LiveCodeBench.

| Model | Easy | Medium | Hard |
| :-- | :--: | :--: | :--: |
| QwQ-32B-Preview | $\mathbf{9 2 . 3}$ | 46.0 | 10.2 |
| START | $\mathbf{9 2 . 3}$ | $\mathbf{8 4 . 6}$ | $\mathbf{1 2 . 2}$ |

attributed to the fact that Physics often necessitate extensive computational reasoning, whereas Biology primarily relies on knowledge-based reasoning. Consequently, the utilization of Python-based tools(START) yields more pronounced efficacy in the former disciplines, while the utilization of internet knowledge(search-o1-32B) works better on the latter.

MATH Benchmarks Performance On the MATH benchmarks, START also demonstrates considerable advantages over QwQ-32B-Preview. Specifically, it achieves absolute improvements of $3.8 \%, 15.0 \%, 16.7 \%$ and $7.1 \%$ on the MATH500,

AMC23, AIME24 and AIME25, respectively. The performance of START is comparable to that of R1-Distill-Qwen-32B, which is distilled from 671B DeepSeek-R1, and overall it exceeds o1preview. These results highlight the significant role of Python-based tools in enhancing mathematical reasoning capabilities.

LiveCodeBench Performace On the LiveCodeBench, START, by equipping the model with the capability to invoke debugging tools, achieves an absolute improvement of $5.9 \%$ over QwQ-32BPreview. We find that START improves the most compared to QwQ-32B-Preview on questions with medium difficulty from 3. The possible reason is that for easy questions, QwQ-32B-Preview can generate the correct answers with high probability without debugging, and for hard questions, based on the current capabilities of the model, a limited number of debugs is also difficult to solve.

### 4.5 Analysis

### 4.5.1 Long CoT vs Long TIR

To ascertain whether our performance gains stem from the additional training questions or from the tool invocation capability, we conduct an experiment using the same set of queries from $D_{S T A R T}$ but only apply RFT with QwQ-32B-Preview. For each query, we sampled 32 responses with a tem-

Table 4: Compare long cot with long tir on challenging reasoning tasks, including PhD-level science QA, math, and code benchmarks. We report Pass@1 metric for all tasks.

| Method | GPQA | MATH500 | AMC23 | AIME24 | AIME25 | LiveCodeBench |
| :-- | :--: | :--: | :--: | :--: | :--: | :--: |
| QwQ-32B-Preview | 58.1 | 90.6 | 80.0 | 50.0 | 40.0 | 41.4 |
| QwQ-RFT | 58.5 | 91.8 | 82.5 | 53.3 | 33.3 | 42.1 |
| START (Ours) | $\mathbf{6 3 . 6 (+ 5 . 5 )}$ | $\mathbf{9 4 . 4 (+ 3 . 8 )}$ | $\mathbf{9 5 . 0}(+15.0)$ | $\mathbf{6 6 . 7}(+16.7)$ | $\mathbf{4 7 . 1}(+7.1)$ | $\mathbf{4 7 . 3}(+5.9)$ |

![img-4.jpeg](img-4.jpeg)

Figure 5: Test time scaling for QwQ-32B-Preview and START on challenge math bench marks via Hint-infer.
perature of 0.7 and a top-p value of 0.95 . After filtering out incorrect responses and responses with repeating strings, we retain at most one response per question and get the long CoT dataset $D_{\mathrm{RFT}}$. Based on $D_{\mathrm{RFT}}$, we fine-tune QwQ-32B-Preview, yielding QwQ-RFT. This methodological approach allows us to isolate the impact of tool invocation from that of the expanded training dataset.

The results presented in Table 4 indicate that the performance of QwQ-RFT is nearly on par with that of QwQ-32B-Preview. Therefore, the observed performance advantage of START is likely predominantly driven by its tool invocation capability, suggesting that this feature plays a critical role in enhancing its effectiveness.

### 4.5.2 Analysis of Hint-infer

Compare QwQ-32B-Preview with Hint-infer and START Through Hint-RFT, we discover that QwQ-32B-Preview inherently possesses the potential to invoke tools, although this capability is challenging to elicit through prompting alone and instead requires explicit hints to activate. START, which is fine-tuned from QwQ-32B-Preview using Hint-RFT, allows us to directly compare the performance of QwQ-32B-Preview with Hint-infer against that of START. To avoid interrupting QwQ-32B-Preview's reasoning process, we only insert hints before the stop token of QwQ-32B-Preview(see more in Appendix A.4). This comparison provides a basis for evaluating the necessity of fine-tuning, as it helps to determine whether the enhanced performance of START is primarily due to the fine-tuning process or can be sufficiently achieved through hint-based prompting alone.

From Table 6, it is evident that incorporating hints during the inference process of QwQ-32BPreview leads to improvements across all benchmarks. However, these improvements are relatively modest compared to the gains achieved by START. Consequently, START, through Hint-RFT, significantly enhances QwQ-32B-Preview's tool invocation capabilities, demonstrating the effectiveness of fine-tuning in unlocking the model's latent potential.

Test-time scaling via Hint By inserting hints at the end of QwQ-32B-Preview's inference process, we can simultaneously increase both the model's thinking time and its accuracy(see Figure 5) by multiple rounds of inserting hint before stop token. It indicates that Hint-infer is a simple yet effective method for achieving sequential test-time scaling. Unlike the strategy outlined in (Muennighoff et al., 2025b), which merely increases the number of "wait" tokens, our method augments the number of tool invocation. The use of Hintinfer for START, on the other hand, does not work as well as on QwQ-32B-Preview, and the reason behind this may be that those hints we inserted were already available during the reasoning process between STARTs, reducing the amount of information in the added hints. More results and analysis are list in A.2.

## 5 Conclusion

this paper presents START, a groundbreaking toolintegrated long Chain-of-Thought reasoning model that effectively mitigates the limitations of existing large reasoning models (LRMs) through the innovative integration of external tools and selflearning techniques. Our contributions, namely Hint-infer and Hint-RFT, showcase a novel approach to enhance reasoning capabilities by enabling LRM to leverage coding interpreters for complex computations and self-debugging. The empirical results demonstrate significant improvements in performance across a range of challenging benchmarks, establishing START as a leading open-source solution for advanced reasoning tasks. By combining long CoT with tool integration, START sets a new standard for the future development of LLMs, paving the way for more reliable and efficient reasoning in higher-level cognitive tasks.

## 6 Limitations

While our work on START demonstrates significant advancements in tool-integrated long Chain-of-Thought reasoning, it is essential to acknowledge several limitations inherent in our approach.

Firstly, our research exclusively focuses on the integration of a Python interpreter as the sole external tool. Although this choice was made for its relevance to many reasoning tasks, we believe that incorporating a wider variety of tools-such as search engines, specialized libraries, or different computational resources-could potentially enhance the model's performance and versatility. Future work could explore how diverse toolsets might contribute to more robust reasoning across various domains.

Secondly, the manual design of hints for insertion into the long CoT reasoning process may inadvertently disrupt the model's original flow of thought. While we aimed to strategically position these hints to optimize performance, the effectiveness of hint positioning and selection could vary based on the specific task or context. More nuanced criteria for determining the most effective types and placement of hints might result in further improvements in reasoning fluidity and accuracy.

Additionally, our empirical evaluations were conducted on a limited set of benchmarks. Although results reported demonstrate promising outcomes, the generalizability of our findings remains to be established across broader and more diverse datasets. The performance of START may be sensitive to variations in task complexity, domain specificity, and the characteristics of the input data.

Lastly, potential risks associated with the misuse of the technology must be considered. The ability of our model to generate code or suggest problem-solving strategies could be inadvertently leveraged for malicious purposes, such as crafting disinformation or automating harmful tasks. It is crucial to implement safeguards and establish ethical guidelines to monitor and mitigate such risks.

In summary, while our research provides a significant step forward, acknowledging these limitations is essential to paving the way for future improvements and ensuring the responsible development and application of tool-integrated reasoning models.

## References

Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, and Dahua Lin. 2024. Internlm2 technical report. Preprint, arXiv:2403.17297.

Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2023. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Trans. Mach. Learn. Res., 2023.

DeepSeek-AI. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.

DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding

Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. 2024. Deepseek-v3 technical report. Preprint, arXiv:2412.19437.

Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. 2023. RAFT: reward ranked finetuning for generative foundation model alignment. Trans. Mach. Learn. Res.

Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani,

Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. 2024. The Llama 3 herd of models. CoRR, abs/2407.21783.

Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. 2024. Tora: A tool-integrated reasoning agent for mathematical problem solving. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024.

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the MATH dataset. In NeurIPS Datasets and Benchmarks.

Huggingface. 2025. Open r1.
Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. 2024. Qwen2.5-Coder technical report. CoRR, abs/2409.12186.

Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. LiveCodeBench: Holistic and contamination free evaluation of large language models for code. CoRR, abs/2403.07974.

Chengpeng Li, Guanting Dong, Mingfeng Xue, Ru Peng, Xiang Wang, and Dayiheng Liu. 2024. Dotamath: Decomposition of thought with code assistance and self-correction for mathematical reasoning. CoRR, abs/2407.04078.

Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. 2024. Numinamath. [https://github.com/project-numina/ aimo-progress-prize](https://github.com/ project-numina/aimo-progress-prize/blob/ main/report/numina_dataset.pdf).

Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025. Search-o1: Agentic search-enhanced large reasoning models. Preprint, arXiv:2501.05366.

Minpeng Liao, Chengxi Li, Wei Luo, Jing Wu, and Kai Fan. 2024. MARIO: math reasoning with code interpreter output - A reproducible pipeline. In ACL (Findings), pages 905-924. Association for Computational Linguistics.

Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Let's verify step by step. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024.

Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025a. s1: Simple test-time scaling. Preprint, arXiv:2501.19393.

Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025b. s1: Simple test-time scaling.

OpenAI. 2024a. Hello GPT-4o.
OpenAI. 2024b. Learning to reason with LLMs.
OpenAI. 2025. Openai o3-mini.
Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, and Pengfei Liu. 2024. O1 replication journey: A strategic progress report - part 1. Preprint, arXiv:2410.18982.

Qwen Team. 2024. QwQ: Reflect deeply on the boundaries of the unknown.

Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models.

David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2023. GPQA: A graduate-level Google-proof Q\&A benchmark. CoRR, abs/2311.12022.

Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. CoRR, abs/2302.04761.

Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300.

Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing

Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and Zonghan Yang. 2025. Kimi k1.5: Scaling reinforcement learning with llms. Preprint, arXiv:2501.12599.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837.

Zijian Wu, Suozhi Huang, Zhejian Zhou, Huaiyuan Ying, Jiayu Wang, Dahua Lin, and Kai Chen. 2024. Internlm2.5-stepprover: Advancing automated theorem proving via expert iteration on large-scale LEAN problems. CoRR, abs/2410.15700.

Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, and Xiaodan Liang. 2024. Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data. CoRR, abs/2405.14333.

An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115.

An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. 2024. Qwen2.5-Math technical report: Toward mathematical expert model via self-improvement. CoRR, abs/2409.12122.

Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025. Limo: Less is more for reasoning. Preprint, arXiv:2502.03387.

Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. 2023. Scaling relationship on learning mathematical reasoning with large language models. Preprint, arXiv:2308.01825.

Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. 2022. Star: Bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022.

## A Appendix

## A. 1 Training set of START

## A. 2 More results about Hint-infer

Building upon the observed trends, the detailed results in Table A. 2 further underscore the efficacy of the QwQ-Hint-infer and START-Hint-infer methods across diverse challenging reasoning tasks. Specifically, for datasets such as aime24, aime25, gpqa, amc23, MATH500, and LiveCodeBench, QwQ-32B-Preview consistently demonstrates performance enhancements with each subsequent round of hint insertion. For instance, aime24 improves from $50.0 \%$ in Round 0 to $60.0 \%$ in Round 3, and MATH500 shows a marginal yet steady increase from $90.6 \%$ to $92.4 \%$ over the same rounds. This consistent upward trend highlights the method's ability to incrementally refine the model's reasoning capabilities through iterative hint integration.

In contrast, the START-Hint-infer approach exhibits a more varied performance across different datasets. While there are improvements in some areas, such as AIME25, where the Pass@1 metric reaches $60.0 \%$ by Round 3 and LiveCodeBench sees an increase from $47.3 \%$ to $50.0 \%$, other datasets like GPQA and LiveCodeBench show relatively modest gains and even no gains. This disparity suggests that the effectiveness of Hint-infer may be contingent on the inherent characteristics of the dataset and the nature of the reasoning tasks involved.

## A. 3 Prompting Methods for Data annotation

We investigated three common methods to trigger existing reasoning LLMs to generate long CoT with Python tool calls in mathematical reasoning tasks. The first method is "direct prompt"(Please integrate natural language reasoning with programs to solve the problem.), which instructs the model to directly use Python tools during reasoning. The second method, "well-designed prompt" is derived from search-o1 (Li et al., 2025) and provides detailed instructions on how to use the tools; this prompt successfully triggers the model to generate special tokens for browser calls in searcho1. The third method is "in-context prompt"(Give some demonstrations in the prompt) which leverages examples to guide the model in generating data in the same format. We do not use general LLMs, as they typically cannot produce long CoTs. For the O1 series, we can only assess whether the

Table 5: Sources of Dataset D

| Source | Quantity |
| :-- | :--: |
| AIME problems (before 2024) | 890 |
| MATH (Hendrycks et al., 2021) | 7500 |
| Numina-MATH (LI et al., 2024) | 28505 |
| Code Data |  |
| Codeforces | 7505 |
| Code contests | 2011 |
| LiveCodeBench (before July 2024) (Jain et al., 2024) | 558 |
| Total | 49969 |

Table 6: Comparison of QWQ-Hint-infer and START-Hint-infer on challenging reasoning tasks, including PhDlevel science QA, math, and code benchmarks. We report Pass@1 metric for all tasks.

| Dataset | QwQ-32B-Preview |  |  |  | START |  |  |  |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|  | Round <br> $\mathbf{0}$ | Round <br> $\mathbf{1}$ | Round <br> $\mathbf{2}$ | Round <br> $\mathbf{3}$ | Round <br> $\mathbf{0}$ | Round <br> $\mathbf{1}$ | Round <br> $\mathbf{2}$ | Round <br> $\mathbf{3}$ |
| aime24 | 50.0\% | 53.3\% | 56.7\% | 60.0\% | 66.7\% | 66.7\% | 66.7\% | 70.0\% |
| aime25 | 40.0\% | 47.1\% | 47.1\% | 53.3\% | 47.1\% | 47.1\% | 60.0\% | 60.0\% |
| gpqa | 58.5\% | 58.6\% | 59.6\% | 59.6\% | 63.6\% | 61.6\% | 60.6\% | 61.6\% |
| amc23 | 80.0\% | 85.0\% | 90.0\% | 92.5\% | 95.0\% | 92.5\% | 95.0\% | 95.0\% |
| MATH500 | 90.6\% | 92.0\% | 92.0\% | 92.4\% | 94.4\% | 95.0\% | 95.6\% | 95.2\% |
| LiveCodeBench | 41.4\% | 42.0\% | 42.0\% | 42.0\% | 47.3\% | 48.2\% | 50.0\% | 50.0\% |

summary includes Python tool invocation. As a result, we found that neither QwQ-32B-Preview, DeepSeek-R1, nor o1-mini could successfully generate long CoTs with code interpreter calls using the three prompt-based methods. In contrast, the hint-infer method was able to trigger the model to produce Python code with $100 \%$ success.

## A. 4 Hint-infer for test time scaling

The three rounds hints of GPQA and MATH for Hint-infer are: Wait, I can use Python to check if my approach is correct and refine it, if necessary. "python, Wait, I need to utilize Python code again to meticulously check to make sure I understand the question correctly as well as reasoning correctly." 'python and Wait, I can think more deeply about this problem through python tools." 'python. Hints of LivecodeBench is the same in Hint-Library. For code problem with starter code, the code template is

```
{startcoder}
# Test the example inputs
solution = Solution()
# Example input1
test_input1 = ...
# Example input2
```

test_input2 = ...
\# Print output
print(solution.function_name(test_input1))
print(solution.function_name(test_input2))
\# Check the output
' ' '
' ' ' output
$[\ldots]$
' ' '

For code problem without starter code, the code template is
def function_name (parameters):
\#Implementation\n
\# Test the example inputs
solution = Solution()
\# Example input1
test_input1 = ...
\# Example input2
test_input2 = ...
\# Print output
print(solution.function_name(test_input1))
print(solution.function_name(test_input2))
\# Check the output
' ' '
' ' ' output
$[\ldots]$

![img-5.jpeg](img-5.jpeg)

# A. 5 Case Study For START 

## A.5.1 A case form AMC23

## A case form AMC23

## Problem:

In the state of Coinland, coins have values 6,10 , and 15 cents. Suppose $x$ is the value in cents of the most expensive item in Coinland that cannot be purchased using these coins with exact change. What is the sum of the digits of $x$ ?

## Solution:

So I've got this problem from the state of Coinland, where they have coins worth 6,10 , and 15 cents. The question is asking for the value of the most expensive item that you can't buy using these coins with exact change. Then, I need to find the sum of the digits of that value.
First off, I need to understand what this is asking. It seems like a problem related to the concept of the Frobenius coin problem, which is about finding the largest amount that cannot be obtained using given coin denominations. In the classic case, with two coprime denominations, there's a formula to find this number, but here we have three denominations: 6,10 , and 15 cents.
I know that for two coprime numbers, $a$ and $b$, the largest number that can't be expressed as a non-negative integer combination of $a$ and $b$ is $a b-a-b$. But here, we have three numbers, and they aren't all pairwise coprime. For instance, 6 and 10 share a common factor of 2 , and 6 and 15 share a common factor of 3 . So, the standard Frobenius number formula doesn't directly apply. Maybe I should start by looking at the greatest common divisor (GCD) of these numbers. Let's find the GCD of 6,10 , and 15 .
The GCD of 6 and 10 is 2 .
The GCD of 2 and 15 is 1 .
So, the overall GCD is 1 . That means that eventually, any sufficiently large number can be expressed as a combination of these coins, since their GCD is 1 .
But, there might be some gaps before we reach a certain point where all numbers can be expressed. Perhaps I should look for the Frobenius number for these three denominations. However, I recall that for more than two denominations, there isn't a general formula for the Frobenius number. So, I might need to approach this problem differently.
One approach could be to consider the possible combinations of these coins and see which numbers can be expressed. Maybe I can start by listing out the possible sums.
Let's list out some combinations:

- 0 coins: 0 cents
- 1 coin: 6,10 , or 15 cents
- 2 coins: $6+6=12,6+10=16,6+15=21,10+10=20,10+15=25,15+15=30$ cents
- 3 coins: $6+6+6=18,6+6+10=22,6+6+15=27,6+10+10=26,6+10+15=31$, $6+15+15=36,10+10+10=30,10+10+15=35,10+15+15=40,15+15+15=45$ cents
- And so on...

This could take a while, and I might miss some numbers. Maybe there's a better way.
Another thought: since the GCD is 1 , I know that every number greater than or equal to a certain number can be expressed as a combination of these coins. I need to find the largest number that cannot be expressed this way.
Perhaps I can look at the problem in terms of modulo arithmetic. For example, since 6 is divisible by 3,10 is not, and 15 is divisible by 3 , I can consider how these combine in terms of modulo 3 . Wait, but that might not be directly helpful. Let me think differently.
I've heard of the concept of the "Frobenius number" for two coins, but for three or more, it's more complicated. Maybe I can reduce this problem to a combination of smaller problems.
Let me consider pairs of these denominations and find their Frobenius numbers, then see how that

# helps. 

First, consider only 6 and 10 cent coins. Since $\operatorname{GCD}(6,10)=2$, which is greater than 1 , the Frobenius number for these two is not defined because you can't make all even numbers greater than a certain point with just 6 and 10. Wait, no. Actually, since their GCD is 2 , you can make any even number greater than or equal to a certain value.
Wait, the Frobenius number formula $a b-a-b$ applies only when $a$ and $b$ are coprime. Here, 6 and 10 are not coprime, so that formula doesn't apply.
Similarly, for 6 and $15, \operatorname{GCD}(6,15)=3$, which is greater than 1 .
And for 10 and $15, \operatorname{GCD}(10,15)=5$.
So, none of the pairs are coprime.
This complicates things.
Alternatively, maybe I can look at the problem in terms of linear combinations.
I need to find the largest $x$ such that there are no non-negative integers $a, b, c$ satisfying $6 a+10 b+$ $15 c=x$.
This seems tricky. Maybe I can look for a pattern in the possible sums.
Let me try to list out the possible sums in order:
$0,6,10,12,15,16,18,20,21,22,24,25,26,27,30$, etc.
Wait, I can use Python to generate specific examples that fit the problem's conditions.

```
Python Code
    from itertools import product
    def can_be_paid(x, coins):
        for a in range(x//coins[0] + 1):
            for b in range(x//coins[1] + 1):
                for c in range(x//coins[2] + 1):
                    if a*coins[0] + b*coins[1] + c*coins[2] == x:
                        return True
        return False
    coins = [6, 10, 15]
    max_x = 100
    # arbitrary upper limit to search for the largest x that cannot be paid
    possible_x = []
    for x in range(1, max_x):
        if not can_be_paid(x, coins):
            possible_x.append(x)
    print((max(possible_x), sum(int(digit) for digit in str(max(possible_x)))))
```


## Output

$(29,11)$

From the code, I found that the largest value that cannot be purchased using coins of 6,10 , and 15 cents is 29 cents. The sum of the digits of 29 is $2+9=11$.
So, the answer is 11 .
**Final Answer**

# A case form AIME24 

## Problem:

Let $p$ be the least prime number for which there exists a positive integer $n$ such that $n^{4}+1$ is divisible by $p^{2}$. Find the least positive integer $m$ such that $m^{4}+1$ is divisible by $p^{2}$.

## Solution:

I'm trying to solve this problem. It says: Let $p$ be the least prime number for which there exists a positive integer $n$ such that $n^{4}+1$ is divisible by $p^{2}$. Find the least positive integer $m$ such that $m^{4}+1$ is divisible by $p^{2}$.
First, I need to find the smallest prime $p$ such that there's some positive integer $n$ making $n^{4}+1$ divisible by $p^{2}$. Then, using that $p$, I have to find the smallest positive integer $m$ where $m^{4}+1$ is divisible by $p^{2}$.
So, to start, I need to understand when $n^{4}+1$ is divisible by $p^{2}$. That means $p^{2}$ divides $n^{4}+1$, or in other words, $n^{4} \equiv-1\left(\bmod p^{2}\right)$.
I know that for primes, certain properties hold due to Fermat's Little Theorem and Euler's Theorem, but I'm not sure how to apply them here directly because we're dealing with $p^{2}$, not just $p$.
Maybe I should look at the equation $n^{4} \equiv-1\left(\bmod p^{2}\right)$. This implies that $n^{8} \equiv 1\left(\bmod p^{2}\right)$, because if $n^{4} \equiv-1$, then squaring both sides gives $n^{8} \equiv 1\left(\bmod p^{2}\right)$.
So, the order of $n$ modulo $p^{2}$ divides 8 , meaning the order is $1,2,4$, or 8 . But since $n^{4} \equiv-1$ $\left(\bmod p^{2}\right)$, the order can't be 1,2 , or 4 , because in those cases, $n^{4}$ would be 1 , not -1 . Therefore, the order must be 8 .
This means that 8 divides the order of the multiplicative group modulo $p^{2}$, which is $\phi\left(p^{2}\right)=$ $p(p-1)$, where $\phi$ is Euler's totient function.
So, 8 divides $p(p-1)$. Since $p$ is prime, there are a few cases to consider:

1. $p=2$
2. $p$ is an odd prime, and 8 divides $p-1$

Let's consider $p=2$ first.
If $p=2$, then $p^{2}=4$. We need to check if there exists an $n$ such that $n^{4}+1$ is divisible by 4 .
Let's compute $n^{4}+1$ modulo 4 :

- If $n$ is even, $n^{4}$ is divisible by 16 , so $n^{4} \equiv 0(\bmod 4)$, thus $n^{4}+1 \equiv 1(\bmod 4)$, which is not divisible by 4 .
- If $n$ is odd, $n^{4} \equiv 1(\bmod 4)$, so $n^{4}+1 \equiv 2(\bmod 4)$, which is also not divisible by 4 .

Therefore, there is no positive integer $n$ such that $n^{4}+1$ is divisible by 4 . So, $p=2$ is not a valid choice.
Next, consider odd primes where 8 divides $p-1$. That is, $p \equiv 1(\bmod 8)$.
The smallest such primes are $17,41,73$, etc.
Alternatively, there could be other primes where $p$ divides $n^{4}+1$, but for $p^{2}$ to divide $n^{4}+1$, it's more restrictive.
Wait a minute, maybe I should consider quadratic residues or higher power residues.
Alternatively, perhaps I should look for primes where -1 is a fourth power residue modulo $p^{2}$.
I recall that for primes, -1 is a quadratic residue if $p \equiv 1(\bmod 4)$, but here we're dealing with fourth powers.
Maybe I should consider the Legendre symbol or higher power residues.
Alternatively, perhaps I can use Hensel's Lemma to lift solutions from modulo $p$ to modulo $p^{2}$.
First, I need to find primes $p$ such that there exists an $n$ with $n^{4} \equiv-1(\bmod p)$, and then check if that solution can be lifted to modulo $p^{2}$.
So, start by finding primes $p$ for which $n^{4} \equiv-1(\bmod p)$ has a solution.
This is equivalent to saying that -1 is a fourth power residue modulo $p$.
The multiplicative group modulo $p$ is cyclic of order $p-1$, so $n^{4} \equiv-1(\bmod p)$ has a solution

if and only if there exists an integer $k$ such that $4 k \equiv \frac{(p-1)}{2}(\bmod p-1)$, since -1 is a square modulo $p$ if $p \equiv 1(\bmod 4)$.
Wait, I can use Python to automate these extensive calculations.

# Python Code 

```
def find_least_p():
    p = 3 # Start with the smallest odd prime
    while True:
        if any(pow(n, 4, p) == p - 1 for n in range(1, p)):
            # Check if n^4 = -1 mod p has a solution
            for n in range(1, p):
                if (n**4) % p == p - 1:
                    # Now check if there exists m such that m^4 = -1 mod p^2
                        p_squared = p * p
                        for m in range(1, p_squared):
                            if (m**4) % p_squared == p_squared - 1:
                            return p, m
    p = next_prime(p)
def next_prime(p):
    # Implement a function to get the next prime after p
    if p == 2:
        return 3
    candidate = p + 2
    while not is_prime(candidate):
        candidate += 2
    return candidate
def is_prime(num):
    # Implement a function to check if a number is prime
    if num <= 1:
        return False
    if num == 2:
        return True
    if num % 2 == 0:
        return False
    for i in range(3, int(num**0.5) + 1, 2):
        if num % i == 0:
            return False
    return True
# Find the least prime p and the least m
p, m = find_least_p()
print((p, m))
```


## Output

SyntaxError: invalid syntax
It seems there was a syntax error in the code due to the way the condition was written. Let's

correct the code and try again.
I need to ensure that the condition for checking if $n^{4} \equiv-1(\bmod p)$ is properly implemented. Also, I need to make sure that the function to find the next prime is correctly defined.
Let's fix the code accordingly.

```
Python Code
def find_least_p():
    p = 3 # Start with the smallest odd prime
    while True:
        # Check if there exists n such that n*4 = -1 mod p
        found_n = False
        for n in range(1, p):
            if (n**4) % p == p - 1:
                found_n = True
                break
            if found_n:
            # Now check if there exists m such that m^4 = -1 mod p^2
            p_squared = p * p
            for m in range(1, p_squared):
                if (m**4) % p_squared == p_squared - 1:
                return p, m
            p = next_prime(p)
def next_prime(p):
    # Implement a function to get the next prime after p
    if p == 2:
        return 3
    candidate = p + 2
    while not is_prime(candidate):
        candidate += 2
    return candidate
```

# Python Code 

```
def is_prime(num):
    # Implement a function to check if a number is prime
    if num <= 1:
        return False
    if num == 2:
        return True
    if num % 2 == 0:
        return False
    for i in range(3, int(num**0.5) + 1, 2):
        if num % i == 0:
            return False
    return True
# Find the least prime p and the least m
p, m = find_least_p()
print((p, m))
```


## Output

$(17,110)$

The code has determined that the least prime $p$ for which there exists a positive integer $n$ such that $n^{4}+1$ is divisible by $p^{2}$ is $p=17$, and the least positive integer $m$ such that $m^{4}+1$ is divisible by $17^{2}=289$ is $m=110$.
To verify this, let's check if $110^{4}+1$ is indeed divisible by 289 .
First, compute $110^{4}$ :

$$
\begin{gathered}
110^{2}=12100 \\
110^{4}=12100^{2}=146,410,000
\end{gathered}
$$

Then, $110^{4}+1=146,410,001$.
Now, divide by 289 :

$$
146,410,001 \div 289
$$

Let's perform the division:

$$
\begin{gathered}
289 \times 506,613=289 \times 500,000=144,500,000 \\
289 \times 6,613=289 \times 6,000=1,734,000 \\
289 \times 613=289 \times 600=173,400 \\
289 \times 13=3,757
\end{gathered}
$$

Adding them up:

---
*[BibTeX citation](start-self-taught-reasoner-with-tools.bib)*
