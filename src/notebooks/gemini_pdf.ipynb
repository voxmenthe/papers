{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import pathlib\n",
    "import httpx\n",
    "import os\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "from io import BytesIO\n",
    "\n",
    "# load GEMINI_API_KEY from .env file\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mDeepSeek_R1.pdf\u001b[m\u001b[m                 \u001b[31mQwen2_5_1M_Technical_Report.pdf\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls ../papers/reasoning/202501/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# doc_url = \"https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf\"\n",
    "\n",
    "# # Retrieve and encode the PDF byte\n",
    "# filepath = pathlib.Path('file.pdf')\n",
    "# filepath.write_bytes(httpx.get(doc_url).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• Others: DeepSeek-R1 also excels in a wide range of tasks, including creative writing,\n",
      "general question answering, editing, summarization, and more. It achieves an impressive\n",
      "length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are-\n",
      "naHard, showcasing its strong ability to intelligently handle non-exam-oriented queries.\n",
      "Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring\n",
      "long-context understanding, substantially outperforming DeepSeek-V3 on long-context\n",
      "benchmarks.\n",
      "\n",
      "2. Approach\n",
      "\n",
      "2.1. Overview\n",
      "\n",
      "Previous work has heavily relied on large amounts of supervised data to enhance model\n",
      "performance. In this study, we demonstrate that reasoning capabilities can be significantly\n",
      "improved through large-scale reinforcement learning (RL), even without using supervised\n",
      "fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with\n",
      "the inclusion of a small amount of cold-start data. In the following sections, we present: (1)\n",
      "DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and\n",
      "(2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of\n",
      "long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to\n",
      "small dense models.\n",
      "\n",
      "2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\n",
      "\n",
      "Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev-\n",
      "idenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works\n",
      "heavily depended on supervised data, which are time-intensive to gather. In this section, we\n",
      "explore the potential of LLMs to develop reasoning capabilities without any supervised data,\n",
      "focusing on their self-evolution through a pure reinforcement learning process. We start with a\n",
      "brief overview of our reinforcement learning algorithm, followed by the presentation of some\n",
      "exciting results, and hope this provides the community with valuable insights.\n",
      "\n",
      "2.2.1. Reinforcement Learning Algorithm\n",
      "\n",
      "Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group\n",
      "Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is\n",
      "typically the same size as the policy model, and estimates the baseline from group scores instead.\n",
      "Specifically, for each question $q$, GRPO samples a group of outputs ${o_1, o_2,…, o_G}$ from the old\n",
      "policy $\\pi_{\\theta_{old}}$ and then optimizes the policy model $\\pi_{\\theta}$ by maximizing the following objective:\n",
      "\n",
      "$I_{GRPO}(\\theta) = E[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{old}}(O|q)]$\n",
      "\\begin{equation}\n",
      "\\frac{1}{G} \\sum_{i=1}^{G} (min (\\frac{\\pi_{\\theta} (o_i|q)}{\\pi_{\\theta_{old}}(o_i|q)} A_i, clip(\\frac{\\pi_{\\theta} (o_i|q)}{\\pi_{\\theta_{old}}(o_i|q)}, 1 - \\epsilon, 1+ \\epsilon) A_i) - \\beta D_{KL}(\\pi_{\\theta}||\\pi_{ref}))\n",
      "\\end{equation}\n",
      "\n",
      "\\begin{equation}\n",
      "D_{KL} (\\pi_{\\theta}||\\pi_{ref}) = -\\frac{\\pi_{ref}(o_i|q)}{\\pi_{\\theta_{old}}(o_i|q)} log \\frac{\\pi_{\\theta}(o_i|q)}{\\pi_{\\theta_{old}}(o_i|q)} - 1,\n",
      "\\end{equation}\n",
      "\n",
      "where $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_i$ is the advantage, computed using a group of\n",
      "rewards ${r_1,r_2, . . ., r_G }$ corresponding to the outputs within each group:\n",
      "\n",
      "\\begin{equation}\n",
      "A_i = \\frac{r_i – mean(\\{r_1, r_2,……,r_G\\})}{std(\\{r_1,r_2,···,r_G\\})}\n",
      "\\end{equation}\n",
      "\n",
      "A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\n",
      "The assistant first thinks about the reasoning process in the mind and then provides the user\n",
      "with the answer. The reasoning process and answer are enclosed within <think> </think> and\n",
      "<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>\n",
      "<answer> answer here </answer>. User: prompt. Assistant:\n",
      "\n",
      "Table 1 | Template for DeepSeek-R1-Zero. prompt will be replaced with the specific reasoning\n",
      "question during training.\n",
      "\n",
      "2.2.2. Reward Modeling\n",
      "\n",
      "The reward is the source of the training signal, which decides the optimization direction of RL.\n",
      "To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two\n",
      "types of rewards:\n",
      "• Accuracy rewards: The accuracy reward model evaluates whether the response is correct.\n",
      "For example, in the case of math problems with deterministic results, the model is required\n",
      "to provide the final answer in a specified format (e.g., within a box), enabling reliable\n",
      "rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be\n",
      "used to generate feedback based on predefined test cases.\n",
      "• Format rewards: In addition to the accuracy reward model, we employ a format reward\n",
      "model that enforces the model to put its thinking process between ‘<think>' and ‘</think>'\n",
      "tags.\n",
      "\n",
      "We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero,\n",
      "because we find that the neural reward model may suffer from reward hacking in the large-scale\n",
      "reinforcement learning process, and retraining the reward model needs additional training\n",
      "resources and it complicates the whole training pipeline.\n",
      "\n",
      "2.2.3. Training Template\n",
      "\n",
      "To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides\n",
      "the base model to adhere to our specified instructions. As depicted in Table 1, this template\n",
      "requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer.\n",
      "We intentionally limit our constraints to this structural format, avoiding any content-specific\n",
      "biases—such as mandating reflective reasoning or promoting particular problem-solving strate-\n",
      "gies to ensure that we can accurately observe the model's natural progression during the\n",
      "reinforcement learning (RL) process.\n",
      "\n",
      "2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\n",
      "\n",
      "Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-\n",
      "R1-Zero on the AIME 2024 benchmark throughout the reinforcement learning (RL) training\n",
      "process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement\n",
      "in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024\n",
      "shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching\n",
      "performance levels comparable to OpenAI-01-0912. This significant improvement highlights the\n",
      "efficacy of our RL algorithm in optimizing the model's performance over time.\n",
      "\n",
      "Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAI's 01-0912\n",
      "models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. read the original PDF\n",
    "reader = PdfReader(\"../papers/reasoning/202501/DeepSeek_R1.pdf\")\n",
    "\n",
    "# 2. pick the pages you want (zero-based indices)\n",
    "pages_to_send = [4, 5]   # e.g. pages 1, 2 and 3\n",
    "\n",
    "writer = PdfWriter()\n",
    "for idx in pages_to_send:\n",
    "    writer.add_page(reader.pages[idx])\n",
    "\n",
    "# 3. write them to a bytes buffer\n",
    "buf = BytesIO()\n",
    "writer.write(buf)\n",
    "buf.seek(0)\n",
    "subset_pdf_bytes = buf.read()\n",
    "\n",
    "# 4. call Gemini with only that subset\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "prompt = \"Extract the full text of this document with figure/table descriptions. Render the text in markdown format, makeing sure to use LaTeX for equations. Also render any mathematical variables or expressions that are present in the text using inline LaTeX. Pay close attention to proper LaTeX formatting including bracket nesting, and understanding the difference between what is mathematical notation, and what is a text string within an equation. Make sure the latex snippets are properly enclosed using dollar signs so that both the inline LaTex and standalone equations are rendered correctly in markdown. Anything enclosed with $$ is a standalone equation, and anything enclosed with $ is an inline equation.\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[\n",
    "      types.Part.from_bytes(data=subset_pdf_bytes,\n",
    "                            mime_type=\"application/pdf\"),\n",
    "      prompt\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = pathlib.Path('../papers/reasoning/202501/DeepSeek_R1.pdf')\n",
    "prompt = \"Extract the full text of the document including detailed descriptions of the figures and tables.\"\n",
    "response = client.models.generate_content(\n",
    "  model=\"gemini-2.0-flash\",\n",
    "  contents=[\n",
    "      types.Part.from_bytes(\n",
    "        data=filepath.read_bytes(),\n",
    "        mime_type='application/pdf',\n",
    "      ),\n",
    "      prompt])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook gemini_pdf.ipynb to python\n",
      "[NbConvertApp] Writing 2632 bytes to gemini_pdf.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert gemini_pdf.ipynb \\\n",
    "  --to python \\\n",
    "  --TemplateExporter.exclude_output=True \\\n",
    "  --TemplateExporter.exclude_input_prompt=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Papers",
   "language": "python",
   "name": "papers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
