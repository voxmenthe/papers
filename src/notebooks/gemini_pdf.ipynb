{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellUniqueIdByVincent": "9e4be"
   },
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import pathlib\n",
    "import httpx\n",
    "import os\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "from io import BytesIO\n",
    "\n",
    "# load GEMINI_API_KEY from .env file\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellUniqueIdByVincent": "234b9"
   },
   "outputs": [],
   "source": [
    "#!ls ../papers/cot/\n",
    "#!ls -lt /Volumes/bdrive/AA_TO_UPLOAD/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellUniqueIdByVincent": "71842"
   },
   "outputs": [],
   "source": [
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# doc_url = \"https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf\"\n",
    "\n",
    "# # Retrieve and encode the PDF byte\n",
    "# filepath = pathlib.Path('file.pdf')\n",
    "# filepath.write_bytes(httpx.get(doc_url).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "9d869"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Reason from Future: Reverse Thought Chain Enhances LLM Reasoning\n",
      "\n",
      "Yinlong Xu¹, Yanzhao Zheng², Shuoshuo Sun²,\n",
      "Shuaihan Huang², Baohua Dong², Hangcheng Zhu², Ruohui Huang², Gang Yu², Hongxia Xu³,⁴*, Jian Wu¹,⁵*\n",
      "¹ College of Computer Science and Technology, Zhejiang University, Hangzhou, China\n",
      "² Alibaba Group, Hangzhou, China\n",
      "³ State Key Laboratory of Transvascular Implantation Devices and TIDRI, Hangzhou, 310009, China\n",
      "⁴ Liangzhu Laboratory and WeDoctor Cloud, Hangzhou, 350000, China\n",
      "⁵ Zhejiang Key Laboratory of Medical Imaging Artificial Intelligence, Hangzhou, 310058, China\n",
      "{xuyinlong, Einstein, Wujian2000}@zju.edu.cn huangshuaihan@outlook.com\n",
      "{zhengyanzhao.zyz, sunshuoshuo.sss, baohua.dbh, linran.lr09, wentong, ruohai}@taobao.com\n",
      "\n",
      "## Abstract\n",
      "It has been demonstrated that carefully designed reasoning paradigms, like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), can enhance the reasoning capabilities of small language models by detailed thinking and extensive thought searching, unbounded branching factors in the searching space create prohibitive reasoning consumption. However these methods fall into the trap of local optimum reasoning, which means the model lacks a global perspective while solving problems. We propose a novel reasoning paradigm called Reason from Future (RFF), which generates reasoning paths by bidirectional reasoning that combines top-down planning with bottom-up reasoning accumulation. The essence of RFF lies in its reverse reasoning mechanism, which prioritizes core logical relationships and imposes goal-oriented constraints on intermediate steps, thereby reducing the searching space and mitigating error accumulation inherent in sequential forward reasoning. Empirical evaluations across diverse experiments demonstrate that RFF outperforms conventional paradigms with higher accuracy and less searching space to solve complex tasks.\n",
      "\n",
      "## 1 Introduction\n",
      "The rapid evolution of large language models (LLMs), fueled by breakthroughs in deep learning architectures and unprecedented datasets, has demonstrated remarkable potential across natural language processing (NLP) and interdisciplinary applications (Lee and Toutanova, 2018; Radford, 2018; Team et al., 2023; Sel et al., 2023). LLMs like ChatGPT (Achiam et al., 2023) and Llama (Dubey et al., 2024) exhibit human-like text generation, multilingual task execution, and emerging logical reasoning. Current scholarly investigations identify their reasoning capacity for problem\n",
      "\n",
      "* Corresponding authors.\n",
      "\n",
      "**Figure 1: Comparison between simple forward reasoning (left) and forward reasoning guided by back reasoning (right).**\n",
      "The figure illustrates two approaches to solving a word problem: \"Question: There are 10 apples and 10 bananas in the blanket, 5 bananas are a little smaller than others. How many fruits in the blanket?\".\n",
      "\n",
      "**Left Panel (Simple Forward Reasoning):**\n",
      "This panel shows a linear, forward-only thought process.\n",
      "1.  **Initial Thought:** \"Try to minus bananas by 5\". An arrow points from this thought to a potential answer.\n",
      "2.  **Answer Box:** \"There are 10 apples and 10 bananas. 5 bananas are a little smaller. So there are 10 + 10 - 5 = 15 fruits. The answer is 15.\"\n",
      "3.  **Final Answer Box (Top Right):** \"The number of fruits is apples + bananas.\" This is the desired final answer format, which is not correctly reached by the incorrect calculation.\n",
      "\n",
      "**Right Panel (Forward Reasoning guided by Back Reasoning):**\n",
      "This panel shows a more structured, goal-oriented thought process.\n",
      "1.  **Initial Thought (Top Right):** \"The number of fruits is apples + bananas\". This is the desired end state, acting as a guide.\n",
      "2.  **Thought towards goal:** \"I see, do apples + bananas to get 20\". An arrow points from the final goal to this step.\n",
      "3.  **Forward Step:** \"Try to do apples + bananas to get 20\". An arrow points from this thought to the \"Answer\" box.\n",
      "4.  **Answer Box:** \"There are 10 apples and 10 bananas. 5 bananas are smaller. So there are 10 + 10 = 20 fruits. The answer is 20.\"\n",
      "5.  **Back Reason Box:** \"The fruits are the number of sum of apples and bananas. Answer: There are 10 apples and 10 bananas. 5 bananas are smaller. So there are 10 + 10 = 20 fruits.\" This box reinforces the correct reasoning by reiterating the final goal and the correct calculation.\n",
      "\n",
      "The figure demonstrates that by starting with the desired outcome (\"Back Reason\") and using it to guide the \"Forward Reason\", the model is less likely to be distracted by irrelevant information (like the \"5 bananas are a little smaller\") and arrives at the correct solution.\n",
      "\n",
      "decomposition as the critical determinant of functional boundaries, enabling industrial automation and academic research applications.\n",
      "\n",
      "Recent studies demonstrate that well-designed reasoning paradigms can significantly enhance LLMs' reasoning ability without additional costly and time-consuming post-training. A seminal work in this area is Chain-of-Thought (CoT) (Wei et al., 2022), which pioneered the novel view that reasoning ability can be improved by designing reasoning prompts, paradigms, and examples. Tree-of-Thought (ToT) (Yao et al., 2024) provides a searching view to enhance the ability of complex reasoning. Progressive-Hint Prompting (PHP) (Zheng et al., 2023) and Cumulative Reasoning (CR) (Zhang et al., 2023) asks the model to generate hints for the question before generating the answer.\n",
      "\n",
      "Although, these reasoning paradigms, breaking down the solution into multiple steps through prompts or spatial search, can enhance the reasoning ability and coherence of the model. They tend to make the model focus on the current state, resulting in lacking explicit guidance from a global understanding of the problem and excessive exploration of redundant information, overthinking, or errors during inference (Boix-Adsera et al., 2023).\n",
      "\n",
      "In contrast, the way human approaches problem-solving is different. Researches have shown that humans begin by building holistic mental modeling when solving complex problems, allowing problem solvers to form a topological framework before focusing on specific details (Spreng et al., 2009; Koban et al., 2021). This kind of cognitive prediction provides dual guidance for the subsequent solution process: forming a \"cognitive road map\" of the solution path at the macro-level, helping to exclude obviously unrelated branches; evaluation criteria are established at the micro-level so that each specific operation remains dynamically calibrated to the end goal. This global awareness allows us to avoid blindly combining superficial details and instead prioritize purposeful, contextually grounded deductions. This suggests that modeling this local-global consistency thinking paradigm might be able to enable LLMs to strategically synthesize information, minimize irrelevant exploration, and align intermediate steps with the overarching goal.\n",
      "\n",
      "Inspired by the maze-solving strategy of backward reasoning, where reversing the path from the endpoint accelerates discovering the solution, we propose a novel reasoning paradigm called Reason-from-Future (RFF) to enhance the reasoning ability of LLMs by adding reverse thinking process to guide the forward reasoning as shown in Figure 1. RFF integrates bidirectional reasoning by alternating between reverse and forward thinking to maintain solution states: the reverse reasoning generates the potential last state of the target state and sets the last state as the new target, then the forward reasoning takes a step toward the new target. The target state serves as a guide to precisely lead the forward reasoning, and the forward reasoning in turn produces more useful information to make the reverse reasoning more reasonable. We evaluate RFF in five datasets: Game of 24 (Yao et al., 2024), GSM8K (Cobbe et al., 2021), ASDiv (Miao et al., 2021), SVAMP (Patel et al., 2021) MATH-500 (Lightman et al., 2023), and demonstrate significant improvements in accuracy over baseline methods. Additionally, RFF reduces the search space by constraining reasoning to target-driven states, demonstrating good efficiency. Our results highlight the potential of bidirectional, goal-aware reasoning to unlock more robust and systematic problem-solving in LLMs.\n",
      "\n",
      "In summary, We introduce RFF, a novel self-planning reasoning paradigm to enhance the reason ability of LLMs. In which, reverse thinking and forward-thinking alternately to obtain a future perspective and narrow the solution-searching space. We conduct experiments involving four datasets to demonstrate the great performance and efficiency of RFF. And we employ two extra experiments by complicating the questions in Game of 24 and GSM8K. The results represent RFF less consuming in larger search spaces and robust thinking in variant problems.\n",
      "\n",
      "## 2 Related Work\n",
      "\n",
      "### 2.1 Chain of Thought Reasoning\n",
      "In the study of complex reasoning tasks, Chain-of-Thought (CoT) (Wei et al., 2022; Wang et al., 2022) prompting has emerged as a pivotal technique for significantly improving the performance of large language models (LLMs) by explicitly generating intermediate reasoning steps. This approach enables the decomposition of problems into structured, stepwise reasoning pathways, demonstrating particular efficacy in mathematical and logical domains. Recent advancements extend CoT through symbolic formalization (e.g., Symbolic CoT (Xu et al., 2024)), which incorporates formal logic systems to enhance both reliability and interpretability by grounding reasoning in rigorous symbolic frameworks. Critical analyses, however, reveal potential limitations where models may exploit computational redundancy rather than genuine reasoning in extended CoT steps, prompting discussions about mechanistic transparency.\n",
      "\n",
      "### 2.2 Search Reasoning\n",
      "In the domain of search-based reasoning for large language models, the Tree-of-Thought (ToT) (Yao et al., 2024) framework introduces backtracking capabilities within multi-path decision structures, enabling systematic exploration of diverse solution trajectories. This approach proves particularly effective for complex tasks requiring iterative hypothesis generation and validation. Monte Carlo Tree Search (MCTS) (Świechowski et al., 2023) strengthens online decision-making robustness\n"
     ]
    }
   ],
   "source": [
    "# 1. read the original PDF\n",
    "reader = PdfReader(\"../papers/cot/ReasonfromFuture-ReverseThoughtChainEnhancesLLMReasoning2506.03673v1.pdf\")\n",
    "\n",
    "# 2. pick the pages you want (zero-based indices)\n",
    "pages_to_send = [0, 1]   # e.g. pages 1, 2 and 3\n",
    "\n",
    "writer = PdfWriter()\n",
    "# for idx in pages_to_send:\n",
    "#     writer.add_page(reader.pages[idx])\n",
    "\n",
    "for page in reader.pages:\n",
    "  writer.add_page(page)\n",
    "\n",
    "# 3. write them to a bytes buffer\n",
    "buf = BytesIO()\n",
    "writer.write(buf)\n",
    "buf.seek(0)\n",
    "subset_pdf_bytes = buf.read()\n",
    "\n",
    "# 4. call Gemini with only that subset\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "prompt = \"Extract the full text of this document with figure/table descriptions. Render the text in markdown format, makeing sure to use LaTeX for equations. Also render any mathematical variables or expressions that are present in the text using inline LaTeX. Pay close attention to proper LaTeX formatting including bracket nesting, and understanding the difference between what is mathematical notation, and what is a text string within an equation. Make sure the latex snippets are properly enclosed using dollar signs so that both the inline LaTex and standalone equations are rendered correctly in markdown. Anything enclosed with $$ is a standalone equation, and anything enclosed with $ is an inline equation. Include full descriptions of any figures and tables in the appropriate places.\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-preview-05-20\",\n",
    "    contents=[\n",
    "      types.Part.from_bytes(data=subset_pdf_bytes,\n",
    "                            mime_type=\"application/pdf\"),\n",
    "      prompt\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../papers/cot/ReasonfromFuture-ReverseThoughtChainEnhancesLLMReasoning2506.03673v1.md\", \"w\") as f:\n",
    "    f.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "327d8"
   },
   "outputs": [],
   "source": [
    "filepath = pathlib.Path('../papers/reasoning/202501/DeepSeek_R1.pdf')\n",
    "prompt = \"Extract the full text of the document including detailed descriptions of the figures and tables.\"\n",
    "response = client.models.generate_content(\n",
    "  model=\"gemini-2.0-flash\",\n",
    "  contents=[\n",
    "      types.Part.from_bytes(\n",
    "        data=filepath.read_bytes(),\n",
    "        mime_type='application/pdf',\n",
    "      ),\n",
    "      prompt])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "f4aef"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "3df80"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellUniqueIdByVincent": "1d46d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook gemini_pdf.ipynb to python\n",
      "[NbConvertApp] Writing 2632 bytes to gemini_pdf.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert gemini_pdf.ipynb \\\n",
    "  --to python \\\n",
    "  --TemplateExporter.exclude_output=True \\\n",
    "  --TemplateExporter.exclude_input_prompt=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "32004"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Papers",
   "language": "python",
   "name": "papers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "vincent": {
   "sessionId": "9b5ea91452a51db6127482c4_2025-06-08T06-27-56-223Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
